<h1>	ForgtAI : International Workshop on Forging Trust in Artificial Intelligence </h1>
 
Establishing and upholding trust in AI systems is an imperative pursuit as Machine Learning becomes an everyday commodity in our lives. The workshop "Forging Trust in Artificial Intelligence" brings together a group of experts and researchers from diverse subfields, converging on the exploration of how transparency, explainability, fairness, and privacy collectively contribute to making machine learning trustworthy. By uniting experts across these pivotal disciplines, this workshop illuminates the best practices that not only enhance the trustworthiness of AI but also reinforce its ethical foundations. Building on the success of the last year’s workshop, which focused on explainability and security in vision AI, this year’s event shifts its attention to the challenges of managing _sequential and multimodal data in NLP and IoT_. These areas are at the forefront of human-AI interaction: large language models (LLMs) already shape how we access information and make decisions,  while smart IoT systems increasingly define automated processes that directly affect not only the industry but also our everyday lives.

This year’s workshop highlights explainability as a cornerstone of trustworthy AI in continuous systems. It tackles the complexities of understanding how neural networks operate, from demystifying the outputs of LLMs to clarifying automated decisions in continuous IoT environments, and eventually providing more tools to improving AI performance.  Ensuring fairness and mitigating biases are also central themes, as these systems increasingly influence critical aspects of human life. By focusing on transparency and ethical practices, the ForgAI Workshop seeks to equip participants with actionable insights and methodologies for building AI systems that inspire confidence and align with societal values.
By aligning with IJCNN’s mission to explore the latest advancements in neural networks, the workshop deepens the conversation around responsible AI development. It emphasizes that explainability is not just a technical challenge but a fundamental requirement for creating AI systems that serve as reliable and ethical partners in human-AI interactions.


The following list includes (but is not limited to) relevant topics that will be addressed within this workshop:  

<h2> Explainability in ML and NLP Models </h2>
 <ul>
              <li>Explainable Natural Language Processing Models</li>
              <li>Visual Explanations for Language Models</li>
              <li>Explainable Large Language Models</li>               
              <li>Interpretable Architectures for Large Language Models</li>
              <li>Techniques for model transparency and interpretability</li>
              <li>Explainability in Multimodal Learning</li>
              <li>Privacy and Security</li>
  </ul> 

<h2> Explainability in AIoT (Artificial Intelligence in IoT)</h2>
 <ul>
              <li>Interpretable Machine Learning Models for IoT Data</li>
              <li>Explainable Anomaly Detection in IoT Networks</li>
              <li>Explainability for Federated Learning in IoTL</li>
              <li>Causal Inference and Explainability in IoT Applications</li>
              <li>Human-Centric Explainability in IoT</li>
              <li>Continual Learning and Explainability</li>
              <li>Explainable Predictive Maintenance</li>

  </ul> 

<h2> Algorithmic Fairness in Machine Learning</h2>
 <ul>
              <li>Fairness Evaluation and Metrics in ML</li>
              <li>Ethical AI Development and Deployment</li>
              <li>Bias Mitigation in AI</li>
              <li>Fairness in Multimodal Learning</li>
              <li>Fairness in Collaborative Learning</li>
              <li>Transparency and Fairness in Federated Learning</li>
              <li>Explainable and Fair AI Models</li>

  </ul> 



<hr>

<h1># Keynote Speaker</h1>
TBA




<hr>

<h1> Paper Submission</h1>

We accept full or short paper submissions. **Full paper submissions (up to 8 pages)** will be considered for publication in the IJCNN 2025 proceedings on the IEEE Xplore Digital Library, and will be oraly presented during the workshop. **Short papers and extended abstracts (up to 4 pages)** will be considered for oral presentations or poster presentations at the workshop; however, these will not be included in the proceedings. Papers must be submitted through the [IJCNN 2025 CMT System](https://cmt3.research.microsoft.com/IJCNN2025/Track/3/Submission/Create). For submission guidelines please check [here](https://2025.ijcnn.org/authors/initial-author-instructions).
## Submission Deadline: 20/03/2025
## Acceptance Notification: 15/04/2025

<hr>

<h1> Organisers</h1>

**Alexandros Iosifidis, Tampere University, Finland**<br>
**Nistor Grozavu, Cergy Paris University/ETIS, France**<br>
**Aikaterini Tzompanaki, Cergy Paris University/ETIS, France**<br>
**Corina Besliu, Technical University of Moldova, Moldova** <br>
**Nicoleta Rogovschi,  Paris Descartes University, France**<br>

<hr>

<h1> Program Committee</h1>

**Hajer Baazaoui, CY Cergy Paris University/ETIS, France**<br>
**Georgios Bouloukakis, Télécom SudParis/IP-Paris, France**<br>
**Luis Galárraga, INRIA/IRISA, France**<br>
**Apostolos Giannoulidis, Aristotle University of Thessaloniki, Greece**<br>
**Michele Linardi,CY Cergy Paris University/ETIS, France**<br> 
**Illia Oleksiienko, Aarhus University, Denmark**<br>
**Nikolaos Passalis, Aristotle University of Thessaloniki, Greece**<br>
**Dimitris Sacharidis, Université Libre de Bruxelles, Belgium**<br>
**Konstantinos Stefanidis, Tampere University, Finland**<br>

<hr>

<h1> Sponsors</h1>
The workshop is organised under the support of the [PANDORA](https://pandora-heu.eu/) European project and the german company [OTRS GmbH](https://otrs.com/de/home/).

<hr>

<h4> Contact: aikaterini.tzompanaki@cyu.fr ; corina.besliu@ati.utm.md </h4>
